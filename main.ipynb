{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from resnet import *\n",
    "from torch.autograd import Variable\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.0.post4\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing and Preprocessing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "==> Building neural model..\n",
      "\n",
      "Epoch: 0\n",
      "Loss: 1.858\n",
      "Loss: 1.745\n",
      "Loss: 1.660\n",
      "Loss: 1.509 | Acc: 46.110% (4611/10000)\n",
      "Saving to checkpoint..\n",
      "\n",
      "Epoch: 1\n",
      "Loss: 1.265\n",
      "Loss: 1.227\n",
      "Loss: 1.188\n",
      "Loss: 1.102 | Acc: 61.190% (6119/10000)\n",
      "Saving to checkpoint..\n",
      "\n",
      "Epoch: 2\n",
      "Loss: 1.019\n",
      "Loss: 0.999\n",
      "Loss: 0.977\n",
      "Loss: 1.037 | Acc: 63.620% (6362/10000)\n",
      "Saving to checkpoint..\n",
      "total training time: 1120 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "############################################### Demo ########################################\n",
    "\n",
    "leaning_rate = 0.1\n",
    "resume = False\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "log_interval = 100 # printing interval\n",
    "epoch = 3 # total training epochs\n",
    "torch.manual_seed(0) # fix the seed\n",
    "\n",
    "\n",
    "# Data\n",
    "print('==> Preparing and Preprocessing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "# Model\n",
    "if resume:\n",
    "    # Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/ckpt-cifar.t7')\n",
    "    net = checkpoint['net']\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "else:\n",
    "    print('==> Building neural model..')\n",
    "    net = ResNet10(block=BasicBlock, activation=F.relu, batch_norm=True, num_classes=10)\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=leaning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.data[0]\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "        if batch_idx%log_interval == 0 and batch_idx > 0:\n",
    "            print ('Loss: %.3f' % (train_loss/(batch_idx+1)) )\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.data[0]\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "    print ('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "        % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving to checkpoint..')\n",
    "        state = {\n",
    "            'net': net,\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt-cifar.t7')\n",
    "        best_acc = acc\n",
    "\n",
    "train_time = 0\n",
    "for epoch in range(start_epoch, start_epoch+epoch):\n",
    "    start_time = time.time()\n",
    "    train(epoch)\n",
    "    end_time = time.time()\n",
    "    train_time += end_time - start_time\n",
    "    test(epoch)\n",
    "print ('total training time: %d seconds' % train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing and Preprocessing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "==> Building neural model..\n",
      "\n",
      "Epoch: 0\n",
      "Loss: 2.208\n",
      "Loss: 2.129\n",
      "Loss: 2.063\n",
      "Loss: 1.760 | Acc: 32.740% (3274/10000)\n",
      "Saving to checkpoint..\n",
      "\n",
      "Epoch: 1\n",
      "Loss: 1.720\n",
      "Loss: 1.704\n",
      "Loss: 1.670\n",
      "Loss: 1.503 | Acc: 44.410% (4441/10000)\n",
      "Saving to checkpoint..\n",
      "\n",
      "Epoch: 2\n",
      "Loss: 1.501\n",
      "Loss: 1.483\n",
      "Loss: 1.459\n",
      "Loss: 1.273 | Acc: 53.190% (5319/10000)\n",
      "Saving to checkpoint..\n",
      "total training time: 950 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "############################################### Cell for removing batch normalization ########################################\n",
    "\n",
    "leaning_rate = 0.1\n",
    "resume = False\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "log_interval = 100 # printing interval\n",
    "epoch = 3 # total training epochs\n",
    "torch.manual_seed(0) # fix the seed\n",
    "\n",
    "\n",
    "# Data\n",
    "print('==> Preparing and Preprocessing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "# Model\n",
    "if resume:\n",
    "    # Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/ckpt-cifar.t7')\n",
    "    net = checkpoint['net']\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "else:\n",
    "    print('==> Building neural model..')\n",
    "    ###################################### fill following line ############################\n",
    "    net = ResNet10(block=BasicBlock, activation=F.relu, batch_norm=False, num_classes=10)\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=leaning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.data[0]\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "        if batch_idx%log_interval == 0 and batch_idx > 0:\n",
    "            print ('Loss: %.3f' % (train_loss/(batch_idx+1)) )\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.data[0]\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "    print ('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "        % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving to checkpoint..')\n",
    "        state = {\n",
    "            'net': net,\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt-cifar.t7')\n",
    "        best_acc = acc\n",
    "\n",
    "train_time = 0\n",
    "for epoch in range(start_epoch, start_epoch+epoch):\n",
    "    start_time = time.time()\n",
    "    train(epoch)\n",
    "    end_time = time.time()\n",
    "    train_time += end_time - start_time\n",
    "    test(epoch)\n",
    "print ('total training time: %d seconds' % train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing and Preprocessing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "==> Building neural model..\n",
      "\n",
      "Epoch: 0\n",
      "Loss: 2.461\n",
      "Loss: 2.352\n",
      "Loss: 2.280\n",
      "Loss: 2.471 | Acc: 20.050% (2005/10000)\n",
      "Saving to checkpoint..\n",
      "\n",
      "Epoch: 1\n",
      "Loss: 1.952\n",
      "Loss: 1.961\n",
      "Loss: 1.944\n",
      "Loss: 2.265 | Acc: 23.440% (2344/10000)\n",
      "Saving to checkpoint..\n",
      "\n",
      "Epoch: 2\n",
      "Loss: 1.822\n",
      "Loss: 1.824\n",
      "Loss: 1.808\n",
      "Loss: 2.922 | Acc: 20.170% (2017/10000)\n",
      "total training time: 1144 seconds\n"
     ]
    }
   ],
   "source": [
    "############################################### Cell for changing the activation function ########################################\n",
    "\n",
    "leaning_rate = 0.1\n",
    "resume = False\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "log_interval = 100 # printing interval\n",
    "epoch = 3 # total training epochs\n",
    "torch.manual_seed(0) # fix the seed\n",
    "\n",
    "\n",
    "# Data\n",
    "print('==> Preparing and Preprocessing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "# Model\n",
    "if resume:\n",
    "    # Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/ckpt-cifar.t7')\n",
    "    net = checkpoint['net']\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "else:\n",
    "    print('==> Building neural model..')\n",
    "    ###################################### fill following line ############################\n",
    "    net = ResNet10(block=BasicBlock, activation=F.sigmoid, batch_norm=True, num_classes=10)\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=leaning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.data[0]\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "        if batch_idx%log_interval == 0 and batch_idx > 0:\n",
    "            print ('Loss: %.3f' % (train_loss/(batch_idx+1)) )\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.data[0]\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "    print ('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "        % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving to checkpoint..')\n",
    "        state = {\n",
    "            'net': net,\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt-cifar.t7')\n",
    "        best_acc = acc\n",
    "\n",
    "train_time = 0\n",
    "for epoch in range(start_epoch, start_epoch+epoch):\n",
    "    start_time = time.time()\n",
    "    train(epoch)\n",
    "    end_time = time.time()\n",
    "    train_time += end_time - start_time\n",
    "    test(epoch)\n",
    "print ('total training time: %d seconds' % train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing and Preprocessing data..\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "==> Building neural model..\n",
      "\n",
      "Epoch: 0\n",
      "Loss: 1.962\n",
      "Loss: 1.837\n",
      "Loss: 1.755\n",
      "Loss: 1.537 | Acc: 43.940% (4394/10000)\n",
      "Saving to checkpoint..\n",
      "\n",
      "Epoch: 1\n",
      "Loss: 1.458\n",
      "Loss: 1.434\n",
      "Loss: 1.409\n",
      "Loss: 1.367 | Acc: 51.070% (5107/10000)\n",
      "Saving to checkpoint..\n",
      "\n",
      "Epoch: 2\n",
      "Loss: 1.274\n",
      "Loss: 1.262\n",
      "Loss: 1.245\n",
      "Loss: 1.361 | Acc: 52.720% (5272/10000)\n",
      "Saving to checkpoint..\n",
      "total training time: 807 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "############################################### Cell for changing to BottleneckBlock ########################################\n",
    "\n",
    "leaning_rate = 0.1\n",
    "resume = False\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "log_interval = 100 # printing interval\n",
    "epoch = 3 # total training epochs\n",
    "torch.manual_seed(0) # fix the seed\n",
    "\n",
    "\n",
    "# Data\n",
    "print('==> Preparing and Preprocessing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "# Model\n",
    "if resume:\n",
    "    # Load checkpoint.\n",
    "    print('==> Resuming from checkpoint..')\n",
    "    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'\n",
    "    checkpoint = torch.load('./checkpoint/ckpt-cifar.t7')\n",
    "    net = checkpoint['net']\n",
    "    best_acc = checkpoint['acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "else:\n",
    "    print('==> Building neural model..')\n",
    "    net = ResNet10(block=BottleneckBlock, activation=F.relu, batch_norm=True, num_classes=10)\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=leaning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training\n",
    "def train(epoch):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = Variable(inputs), Variable(targets)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.data[0]\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "        if batch_idx%log_interval == 0 and batch_idx > 0:\n",
    "            print ('Loss: %.3f' % (train_loss/(batch_idx+1)) )\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.data[0]\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.data).cpu().sum()\n",
    "\n",
    "    print ('Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "        % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving to checkpoint..')\n",
    "        state = {\n",
    "            'net': net,\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt-cifar.t7')\n",
    "        best_acc = acc\n",
    "\n",
    "train_time = 0\n",
    "for epoch in range(start_epoch, start_epoch+epoch):\n",
    "    start_time = time.time()\n",
    "    train(epoch)\n",
    "    end_time = time.time()\n",
    "    train_time += end_time - start_time\n",
    "    test(epoch)\n",
    "print ('total training time: %d seconds' % train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
